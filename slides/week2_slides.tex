\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}

\title{Week 2: Statistical Analysis \& Feature Importance}
\subtitle{Heart Failure Survival Analysis}
\author{MDST Project}
\date{Winter 2026}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

%=============================================================================
\section{Week 1 Recap}
%=============================================================================

\begin{frame}{Quick Recap: Week 1 - Exploratory Data Analysis}
\begin{columns}
\column{0.5\textwidth}
\textbf{Dataset Overview:}
\begin{itemize}
    \item 299 heart failure patients
    \item 13 features (12 predictors + target)
    \item Target: DEATH\_EVENT (0/1)
    \item 68\% survived, 32\% died
    \item No missing values
\end{itemize}

\column{0.5\textwidth}
\textbf{Feature Types:}
\begin{itemize}
    \item \textbf{Continuous:} age, ejection\_fraction, serum\_creatinine, serum\_sodium, time, platelets, CPK
    \item \textbf{Binary:} anaemia, diabetes, high\_blood\_pressure, sex, smoking
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{Week 1 Observations (Visual)}
\textbf{What we saw in the plots:}

\vspace{0.3cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{Differences between groups:}
\begin{itemize}
    \item Died: \textbf{Higher} serum\_creatinine
    \item Died: \textbf{Lower} ejection\_fraction
    \item Died: \textbf{Shorter} follow-up time
    \item Died: \textbf{Older} patients
\end{itemize}

\column{0.5\textwidth}
\textbf{No obvious differences:}
\begin{itemize}
    \item Diabetes (similar in both)
    \item Sex (similar in both)
    \item Smoking (similar in both)
\end{itemize}
\end{columns}

\vspace{0.5cm}
\textbf{This Week's Goal:} Use \textbf{statistical tests} to confirm these observations with numbers!
\end{frame}

\begin{frame}{From Visualization to Statistics}
\begin{table}
\centering
\begin{tabular}{p{5cm}p{5cm}}
\toprule
\textbf{Week 1: EDA} & \textbf{Week 2: Statistics} \\
\midrule
``The boxplots look different'' & ``The difference is statistically significant (p $<$ 0.05)'' \\
\midrule
``This feature seems important'' & ``Random Forest ranks it \#1'' \\
\midrule
``These features look correlated'' & ``VIF = 1.3, no multicollinearity'' \\
\bottomrule
\end{tabular}
\end{table}

\vspace{0.3cm}
\textbf{Key Insight:} Visualization suggests, statistics confirms!
\end{frame}

%=============================================================================
\section{The Lady Tasting Tea}
%=============================================================================

\begin{frame}{A Summer Afternoon in Cambridge, 1920s}
\begin{center}
\Large\textit{``I can tell whether the milk was poured first, or the tea.''}
\end{center}

\vspace{0.5cm}
\textbf{The Scene:}\\
A group of scientists and academics gathered for afternoon tea at Cambridge University. Among them was Dr. Muriel Bristol, an algologist (scientist who studies algae).

\vspace{0.5cm}
When offered a cup of tea, she politely declined -- insisting that she preferred her tea prepared with \textbf{milk poured into the cup first}, before the tea.

\vspace{0.5cm}
\begin{center}
\textit{``I can taste the difference,''} she claimed.
\end{center}

\vspace{0.3cm}
The scientists laughed. \textbf{Surely that's impossible!}\\
The order of mixing couldn't possibly affect the taste... could it?
\end{frame}

\begin{frame}{The Challenge}
\begin{center}
\Large\textbf{``Prove it.''}
\end{center}

\vspace{0.5cm}
The room fell silent. How could they test such a claim?

\vspace{0.5cm}
\begin{itemize}
    \item If she guesses correctly once, is that proof? \textit{(Could be luck...)}
    \item What if she gets 2 right? 3 right? \textit{(Still could be luck...)}
    \item How many correct answers would \textbf{convince} them she has real ability?
\end{itemize}

\vspace{0.5cm}
\begin{center}
\textbf{This is the fundamental question of statistics:}\\[0.3cm]
\Large\textit{How do we distinguish real effects from random chance?}
\end{center}
\end{frame}

\begin{frame}{Enter: Ronald Fisher}
\begin{columns}
\column{0.35\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{fisher.jpg}

\vspace{0.2cm}
\textbf{Sir Ronald A. Fisher}\\
\small(1890--1962)
\end{center}

\column{0.65\textwidth}
One man at the table saw this as more than a parlor game.

\vspace{0.3cm}
\textbf{Ronald Fisher} -- a young statistician working at the Rothamsted Experimental Station -- realized this simple question about tea contained a profound scientific problem.

\vspace{0.3cm}
\textit{``I can design an experiment to test this,''} he said.

\vspace{0.3cm}
What followed would revolutionize science forever.
\end{columns}
\end{frame}

\begin{frame}{Who Was Ronald Fisher?}
\begin{columns}
\column{0.5\textwidth}
\textbf{The ``Father of Modern Statistics''}

\vspace{0.3cm}
\begin{itemize}
    \item Born in London, 1890
    \item Studied mathematics at Cambridge
    \item Poor eyesight prevented WWI service
    \item Worked on agricultural experiments
    \item Revolutionized scientific methodology
\end{itemize}

\column{0.5\textwidth}
\textbf{His Contributions:}
\begin{itemize}
    \item \textbf{P-value} -- probability under null
    \item \textbf{Null hypothesis} -- default assumption
    \item \textbf{ANOVA} -- comparing groups
    \item \textbf{Maximum likelihood estimation}
    \item \textbf{Experimental design principles}
\end{itemize}
\end{columns}

\vspace{0.5cm}
\begin{center}
\small\textit{``To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination.''} -- R.A. Fisher
\end{center}
\end{frame}

\begin{frame}{Fisher's Elegant Experiment}
\textbf{The Design:}
\begin{enumerate}
    \item Prepare \textbf{8 cups} of tea: 4 with milk first, 4 with tea first
    \item Present them in \textbf{random order}
    \item Tell the lady there are exactly 4 of each type
    \item Ask her to identify which 4 cups had milk added first
\end{enumerate}

\vspace{0.5cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{The Mathematics:}
\begin{itemize}
    \item Total ways to choose 4 from 8:\\
    $\binom{8}{4} = \frac{8!}{4! \times 4!} = 70$
    \item Only \textbf{1 way} to get all 4 correct
    \item P(perfect by chance) = $\frac{1}{70} = 1.4\%$
\end{itemize}

\column{0.5\textwidth}
\textbf{Fisher's Reasoning:}\\
If she gets all 4 correct, there's only a 1.4\% chance she was guessing.

\vspace{0.3cm}
This is small enough to \textbf{reject} the idea that she's just lucky!
\end{columns}
\end{frame}

\begin{frame}{The Birth of Hypothesis Testing}
\begin{center}
\Large\textbf{Fisher formalized this into a framework:}
\end{center}

\vspace{0.5cm}
\begin{enumerate}
    \item \textbf{Null Hypothesis ($H_0$):} The lady has no ability (just guessing)
    \item \textbf{Alternative Hypothesis ($H_1$):} The lady has real ability
    \item \textbf{P-value:} Probability of the result if $H_0$ is true
    \item \textbf{Decision:} If p-value $< 0.05$, reject $H_0$
\end{enumerate}

\vspace{0.5cm}
\begin{center}
\fbox{\parbox{0.8\textwidth}{
\centering
\textbf{The Result:} Dr. Bristol correctly identified \textbf{all 8 cups!}\\[0.2cm]
P-value = 1.4\% $<$ 5\%\\[0.2cm]
\textit{She really could taste the difference.}
}}
\end{center}
\end{frame}

\begin{frame}{From Tea Cups to Heart Failure}
\textbf{The exact same logic applies to our analysis:}

\vspace{0.3cm}
\begin{table}
\centering
\begin{tabular}{p{5.5cm}p{5.5cm}}
\toprule
\textbf{Lady Tasting Tea} & \textbf{Heart Failure Analysis} \\
\midrule
Can she distinguish milk-first from tea-first? & Can serum creatinine distinguish survivors from non-survivors? \\
\midrule
$H_0$: She's guessing randomly & $H_0$: No difference between groups \\
\midrule
If p $<$ 0.05 $\rightarrow$ real ability & If p $<$ 0.05 $\rightarrow$ feature matters \\
\bottomrule
\end{tabular}
\end{table}

\vspace{0.5cm}
\textbf{Fisher's Legacy:}
\begin{itemize}
    \item These methods are now used in \textbf{medicine}, \textbf{biology}, \textbf{psychology}, and \textbf{data science}
    \item Every ``p $<$ 0.05'' you see in research traces back to Fisher
\end{itemize}
\end{frame}

%=============================================================================
\section{Correlation Analysis}
%=============================================================================

\begin{frame}{What is Correlation?}
\textbf{Pearson Correlation} measures the \textbf{linear relationship} between two variables.

\vspace{0.3cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{Range: $-1$ to $+1$}
\begin{itemize}
    \item $+1$: Perfect positive (both increase together)
    \item $0$: No linear relationship
    \item $-1$: Perfect negative (one up, other down)
\end{itemize}

\column{0.5\textwidth}
\textbf{Formula:}
$$r = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2 \sum(y_i - \bar{y})^2}}$$
\end{columns}

\vspace{0.3cm}
\textbf{Key Question:} Which features are correlated with death?
\end{frame}

\begin{frame}{Feature Correlations with DEATH\_EVENT}
\begin{columns}
\column{0.5\textwidth}
\textbf{Positive Correlations:}
\begin{itemize}
    \item serum\_creatinine: $+0.29$
    \item age: $+0.25$
\end{itemize}
Higher values $\rightarrow$ higher death risk

\vspace{0.5cm}
\textbf{Negative Correlations:}
\begin{itemize}
    \item time: $-0.53$
    \item ejection\_fraction: $-0.27$
    \item serum\_sodium: $-0.20$
\end{itemize}
Higher values $\rightarrow$ lower death risk

\column{0.5\textwidth}
\textbf{Key Insight:}\\
Follow-up time has the strongest correlation -- but this is expected!

\vspace{0.3cm}
Patients who die have shorter follow-up periods.

\vspace{0.3cm}
\textbf{Warning:} `time` is a ``leaky'' feature -- we wouldn't know it in advance for prediction!
\end{columns}
\end{frame}

%=============================================================================
\section{Statistical Tests}
%=============================================================================

\begin{frame}{T-Test: Comparing Group Means}
\textbf{Question:} Is there a significant difference in feature values between survivors and non-survivors?

\vspace{0.3cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{How it works:}
\begin{enumerate}
    \item Calculate mean for each group
    \item Measure the difference
    \item Account for variability
    \item Get a p-value
\end{enumerate}

\column{0.5\textwidth}
\textbf{Assumptions:}
\begin{itemize}
    \item Normal distribution
    \item Independent samples
    \item (Welch's t-test relaxes equal variance)
\end{itemize}
\end{columns}

\vspace{0.3cm}
\textbf{Null Hypothesis:} No difference between group means ($\mu_1 = \mu_2$)
\end{frame}

\begin{frame}{Mann-Whitney U Test: Non-Parametric Alternative}
\textbf{When to use:} Data is NOT normally distributed

\vspace{0.3cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{How it works:}
\begin{enumerate}
    \item Rank all values together
    \item Sum ranks for each group
    \item Compare rank sums
\end{enumerate}

\textbf{Advantages:}
\begin{itemize}
    \item No normality assumption
    \item Robust to outliers
    \item Works with ordinal data
\end{itemize}

\column{0.5\textwidth}
\textbf{Example:}\\
Age values: [55, 60, 65, 70, 75]\\
Ranks: [1, 2, 3, 4, 5]

\vspace{0.3cm}
If survivors have mostly low ranks and non-survivors have high ranks, there's a significant difference.
\end{columns}
\end{frame}

\begin{frame}{Significant Features (p $<$ 0.05)}
\begin{table}
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Feature} & \textbf{T-test p-value} & \textbf{Mann-Whitney p-value} & \textbf{Significant?} \\
\midrule
time & $2.3 \times 10^{-22}$ & $6.9 \times 10^{-21}$ & Yes \\
ejection\_fraction & $9.6 \times 10^{-6}$ & $7.4 \times 10^{-7}$ & Yes \\
serum\_creatinine & $6.4 \times 10^{-5}$ & $1.6 \times 10^{-10}$ & Yes \\
age & $4.7 \times 10^{-5}$ & $1.7 \times 10^{-4}$ & Yes \\
serum\_sodium & $1.9 \times 10^{-3}$ & $2.9 \times 10^{-4}$ & Yes \\
\midrule
diabetes & 0.97 & 0.97 & No \\
sex & 0.94 & 0.94 & No \\
smoking & 0.83 & 0.83 & No \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

%=============================================================================
\section{Multiple Testing Correction}
%=============================================================================

\begin{frame}{The Problem with Multiple Testing}
\textbf{Scenario:} Testing 12 features at $\alpha = 0.05$

\vspace{0.3cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{The Math:}
\begin{itemize}
    \item Each test: 5\% chance of false positive
    \item 12 tests: Expected false positives $= 12 \times 0.05 = 0.6$
    \item Probability of at least one false positive:\\
    $1 - (0.95)^{12} = 46\%$
\end{itemize}

\column{0.5\textwidth}
\textbf{Real-World Problem:}
\begin{itemize}
    \item Publish 20 studies
    \item 1 will show ``significant'' result by chance
    \item This is why many studies don't replicate!
\end{itemize}
\end{columns}

\vspace{0.5cm}
\textbf{Solution:} Adjust p-values to control the False Discovery Rate (FDR)
\end{frame}

\begin{frame}{Benjamini-Hochberg (FDR) Correction}
\textbf{Goal:} Control the expected proportion of false discoveries

\vspace{0.3cm}
\textbf{Procedure:}
\begin{enumerate}
    \item Rank p-values from smallest to largest: $p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(m)}$
    \item For each p-value, calculate the adjusted value:
    $$p_{adj} = p \times \frac{m}{\text{rank}}$$
    \item Compare adjusted p-values to $\alpha$
\end{enumerate}

\vspace{0.3cm}
\textbf{Why FDR over Bonferroni?}
\begin{itemize}
    \item Bonferroni: Very conservative ($\alpha / m$) -- may miss real effects
    \item FDR: Allows some false positives, but controls the rate
\end{itemize}
\end{frame}

\begin{frame}{After FDR Correction}
\textbf{Still significant (FDR $<$ 0.05):}
\begin{itemize}
    \item time, ejection\_fraction, age, serum\_creatinine, serum\_sodium
\end{itemize}

\vspace{0.3cm}
\textbf{Not significant after correction:}
\begin{itemize}
    \item high\_blood\_pressure, anaemia, diabetes, platelets, sex, smoking, creatinine\_phosphokinase
\end{itemize}

\vspace{0.5cm}
\textbf{Conclusion:} Our 5 significant features remain significant even after accounting for multiple testing!
\end{frame}

%=============================================================================
\section{Feature Importance}
%=============================================================================

\begin{frame}{Statistical Tests vs. Feature Importance}
\begin{table}
\centering
\begin{tabular}{p{5cm}p{5cm}}
\toprule
\textbf{Statistical Tests} & \textbf{Feature Importance} \\
\midrule
``Is there a difference between groups?'' & ``How useful for prediction?'' \\
\midrule
Tests one feature at a time & Considers all features together \\
\midrule
Measures statistical significance & Measures predictive power \\
\midrule
Needs assumptions (normality, etc.) & Often model-based \\
\bottomrule
\end{tabular}
\end{table}

\vspace{0.3cm}
\textbf{Key Insight:} A feature can be statistically significant but not useful for prediction (and vice versa). We need BOTH perspectives!
\end{frame}

\begin{frame}{What is Random Forest?}
\textbf{Random Forest} = A collection of many decision trees that ``vote'' together

\vspace{0.3cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{Analogy:}\\
Like asking 100 doctors for their opinion:
\begin{itemize}
    \item Each doctor (tree) looks at different aspects
    \item They all vote on the outcome
    \item Majority vote wins
\end{itemize}

\column{0.5\textwidth}
\textbf{Why Random Forest?}
\begin{itemize}
    \item Automatically learns useful features
    \item Captures non-linear relationships
    \item Robust and widely used
    \item Provides feature importance for free!
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{Gini Importance (Mean Decrease in Impurity)}
\textbf{How it works:}
\begin{enumerate}
    \item At each split, the tree asks: ``Which feature best separates survived vs. died?''
    \item Features that create better splits are used more often
    \item Importance = how much each feature reduces ``impurity'' (mixing of classes)
\end{enumerate}

\vspace{0.3cm}
\textbf{Simple Example:}
\begin{itemize}
    \item If \texttt{ejection\_fraction < 30} perfectly separates patients $\rightarrow$ HIGH importance
    \item If \texttt{smoking} doesn't help separate groups $\rightarrow$ LOW importance
\end{itemize}

\vspace{0.3cm}
\textbf{Limitation:} Can be biased toward features with many unique values
\end{frame}

\begin{frame}{Permutation Importance: A Better Alternative}
\textbf{Problem with Gini:} Biased toward features with many values

\vspace{0.3cm}
\textbf{Permutation Importance Solution:}
\begin{enumerate}
    \item Train model and measure accuracy
    \item Randomly shuffle one feature's values
    \item Measure how much accuracy drops
    \item Bigger drop = more important feature
\end{enumerate}

\vspace{0.3cm}
\textbf{Intuition:} If shuffling a feature hurts the model a lot, that feature was important!

\vspace{0.3cm}
\textbf{Advantages:}
\begin{itemize}
    \item Unbiased
    \item Works with any model
    \item Evaluated on test data (more reliable)
\end{itemize}
\end{frame}

\begin{frame}{Feature Importance Results}
\textbf{Top 5 Most Important Features:}
\begin{enumerate}
    \item \textbf{time} -- Follow-up period (but ``leaky''!)
    \item \textbf{serum\_creatinine} -- Kidney function indicator
    \item \textbf{ejection\_fraction} -- Heart pumping efficiency
    \item \textbf{age} -- Patient age
    \item \textbf{serum\_sodium} -- Electrolyte balance
\end{enumerate}

\vspace{0.3cm}
\textbf{Clinical Interpretation:}
\begin{itemize}
    \item \textbf{Serum creatinine}: High levels indicate kidney dysfunction
    \item \textbf{Ejection fraction}: Low values mean heart isn't pumping efficiently
    \item These match the original research paper findings!
\end{itemize}
\end{frame}

%=============================================================================
\section{Feature Variance \& Multicollinearity}
%=============================================================================

\begin{frame}{Why Does Feature Variance Matter?}
\textbf{Core Idea:} A feature that doesn't vary can't help distinguish groups!

\vspace{0.3cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{Example:}
\begin{itemize}
    \item If ALL patients have \texttt{diabetes = 1}
    \item This feature tells us nothing
    \item Can't distinguish survivors from non-survivors
\end{itemize}

\column{0.5\textwidth}
\textbf{Variance Formula:}
$$\text{Var}(X) = \frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2$$

Measures spread around the mean
\end{columns}

\vspace{0.3cm}
\textbf{Rule:} Remove features with very low variance (they carry no information)
\end{frame}

\begin{frame}{Coefficient of Variation (CV)}
\textbf{Problem:} Raw variance depends on scale
\begin{itemize}
    \item Platelets: variance = $9.5 \times 10^9$ (large numbers!)
    \item Anaemia: variance = 0.25 (binary 0/1)
    \item Can't compare directly!
\end{itemize}

\vspace{0.5cm}
\textbf{Solution: Coefficient of Variation}
$$CV = \frac{\text{Standard Deviation}}{\text{Mean}} = \frac{\sigma}{\mu}$$

\begin{itemize}
    \item Scale-free measure of relative spread
    \item CV $>$ 1: High variability relative to mean
    \item CV $<$ 1: Low variability relative to mean
\end{itemize}
\end{frame}

\begin{frame}{What is Multicollinearity?}
\textbf{Definition:} When two or more features are highly correlated

\vspace{0.3cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{Simple Example:}
\begin{itemize}
    \item Height in cm
    \item Height in inches
    \item Same information!
\end{itemize}

\textbf{In Our Data:}
\begin{itemize}
    \item sex and smoking: $r = 0.45$
    \item (Men smoke more in this dataset)
\end{itemize}

\column{0.5\textwidth}
\textbf{Why is it a problem?}
\begin{itemize}
    \item Redundant information
    \item Hard to interpret importance
    \item Unstable regression coefficients
    \item Wastes model capacity
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{Variance Inflation Factor (VIF)}
\textbf{VIF} measures how much a feature's variance is ``inflated'' by correlation with others

\vspace{0.3cm}
\textbf{How it works:}
\begin{enumerate}
    \item For feature $X_j$, predict it using ALL other features
    \item Calculate $R^2_j$ (how well others predict $X_j$)
    \item VIF formula: $\text{VIF}_j = \frac{1}{1 - R^2_j}$
\end{enumerate}

\vspace{0.3cm}
\textbf{Intuition:}
\begin{itemize}
    \item If $X_j$ predicted perfectly by others ($R^2 = 1$): VIF $\rightarrow \infty$
    \item If $X_j$ is independent ($R^2 = 0$): VIF $= 1$
\end{itemize}
\end{frame}

\begin{frame}{VIF Interpretation Guide}
\begin{table}
\centering
\begin{tabular}{cl}
\toprule
\textbf{VIF Value} & \textbf{Interpretation \& Action} \\
\midrule
VIF = 1 & No correlation -- no action needed \\
1 $<$ VIF $<$ 5 & Moderate -- usually acceptable \\
5 $\leq$ VIF $<$ 10 & High -- investigate further \\
VIF $\geq$ 10 & Severe -- consider removing feature \\
\bottomrule
\end{tabular}
\end{table}

\vspace{0.5cm}
\textbf{What to do with high VIF?}
\begin{itemize}
    \item Remove one of the correlated features
    \item Combine features (e.g., PCA)
    \item Use regularization (Ridge, Lasso)
\end{itemize}
\end{frame}

\begin{frame}[fragile]{VIF in Python}
\begin{verbatim}
from statsmodels.stats.outliers_influence
     import variance_inflation_factor
from sklearn.preprocessing import StandardScaler

# Scale features for numerical stability
X_scaled = StandardScaler().fit_transform(X)

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data['Feature'] = X.columns
vif_data['VIF'] = [
    variance_inflation_factor(X_scaled, i)
    for i in range(X_scaled.shape[1])
]
vif_data.sort_values('VIF', ascending=False)
\end{verbatim}
\end{frame}

\begin{frame}{VIF Results for Our Dataset}
\begin{columns}
\column{0.5\textwidth}
\begin{table}
\small
\begin{tabular}{lcc}
\toprule
\textbf{Feature} & \textbf{VIF} & \textbf{Status} \\
\midrule
sex & 1.35 & OK \\
smoking & 1.32 & OK \\
age & 1.15 & OK \\
time & 1.14 & OK \\
serum\_sodium & 1.13 & OK \\
ejection\_fraction & 1.10 & OK \\
serum\_creatinine & 1.08 & OK \\
\bottomrule
\end{tabular}
\end{table}

\column{0.5\textwidth}
\textbf{Good news!}
\begin{itemize}
    \item All VIF values are close to 1
    \item No severe multicollinearity
    \item All features provide relatively independent information
\end{itemize}

\vspace{0.3cm}
\textbf{Note:} sex and smoking have slightly higher VIF due to their correlation (0.45)
\end{columns}
\end{frame}

\begin{frame}{Variance vs. VIF: Key Differences}
\begin{table}
\centering
\begin{tabular}{p{5cm}p{5cm}}
\toprule
\textbf{Variance} & \textbf{VIF} \\
\midrule
Measures spread of a \textit{single} feature & Measures correlation \textit{between} features \\
\midrule
Low variance = feature doesn't vary much & High VIF = feature is redundant \\
\midrule
Question: ``Does this feature have different values?'' & Question: ``Is this feature's info already captured by others?'' \\
\midrule
Solution: Remove low-variance features & Solution: Remove one of correlated pair \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

%=============================================================================
\section{Summary}
%=============================================================================

\begin{frame}{Key Takeaways}
\begin{enumerate}
    \item \textbf{Correlation} measures linear relationships between variables
    \item \textbf{T-test} compares means (assumes normality)
    \item \textbf{Mann-Whitney U} is non-parametric (no normality assumption)
    \item \textbf{FDR Correction} controls false discovery rate in multiple testing
    \item \textbf{Random Forest} provides feature importance scores
    \item \textbf{Permutation Importance} is more reliable than Gini importance
    \item \textbf{Variance/CV} tells us how much features vary
    \item \textbf{VIF} detects multicollinearity (redundant features)
\end{enumerate}
\end{frame}

\begin{frame}{Key Findings for Heart Failure Dataset}
\textbf{Most Important Predictive Features:}
\begin{enumerate}
    \item time (but leaky!)
    \item serum\_creatinine
    \item ejection\_fraction
    \item age
    \item serum\_sodium
\end{enumerate}

\vspace{0.3cm}
\textbf{Data Quality:}
\begin{itemize}
    \item No multicollinearity issues (all VIF $<$ 5)
    \item All features have adequate variance
    \item Results match the original research paper!
\end{itemize}
\end{frame}

\begin{frame}{Next Week: Unsupervised Learning}
\begin{itemize}
    \item \textbf{PCA} (Principal Component Analysis)
    \begin{itemize}
        \item Dimensionality reduction
        \item Visualizing high-dimensional data
    \end{itemize}
    \item \textbf{Clustering}
    \begin{itemize}
        \item Finding natural groupings in data
        \item K-means, hierarchical clustering
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Exercises}
\begin{enumerate}
    \item Write a function to return features with significant p-values given a threshold
    \item Implement Mann-Whitney U test for all features
    \item Compare feature rankings from t-test vs. Random Forest
    \item Calculate VIF for all features and interpret results
    \item Write a low-variance filter function
\end{enumerate}

\vspace{0.3cm}
\textbf{Resources:}
\begin{itemize}
    \item Scipy Stats: \url{https://docs.scipy.org/doc/scipy/reference/stats.html}
    \item Statsmodels VIF: \url{https://www.statsmodels.org/}
    \item Scikit-learn: \url{https://scikit-learn.org/}
\end{itemize}
\end{frame}

\end{document}
